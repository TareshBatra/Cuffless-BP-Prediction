{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sd0x_gdlXlpV"
      },
      "source": [
        "# Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-24T10:13:23.504370Z",
          "start_time": "2021-11-24T10:13:20.401834Z"
        },
        "id": "cxzEA4cf9_Zf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, ConvLSTM2D, TimeDistributed, RepeatVector, Flatten, Conv2D, MaxPooling2D, Dropout, Reshape, LeakyReLU, Conv1D, MaxPooling1D\n",
        "from tensorflow.keras import initializers\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import math\n",
        "import glob\n",
        "import scipy\n",
        "!pip install tensorflow_datasets\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from statistics import stdev\n",
        "\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import find_peaks\n",
        "from scipy import interpolate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-24T10:13:23.510002Z",
          "start_time": "2021-11-24T10:13:23.506115Z"
        },
        "id": "2hVi3czgTNlT"
      },
      "outputs": [],
      "source": [
        "print(30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-24T10:13:23.515386Z",
          "start_time": "2021-11-24T10:13:23.511895Z"
        },
        "id": "xsTgTzmLLwQ0"
      },
      "outputs": [],
      "source": [
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-24T10:13:23.656790Z",
          "start_time": "2021-11-24T10:13:23.516930Z"
        },
        "id": "vkdp7xmnMmDX"
      },
      "outputs": [],
      "source": [
        "!python3 --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "mIY7ZiqMBIwK"
      },
      "source": [
        "# Data Pre-Processing\n",
        "\n",
        "The Pre-Processed Data can be found at \"Data/75.pkl\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "xpfQH0HpqGdM"
      },
      "source": [
        "## Dataset Used\n",
        "\n",
        "University of California, Irvine (UCI) crated a Machine learning Repository dataset, derived from the largest publically availble database ‘Multi-parameter Intelligent Monitoring in Intensive Care (MIMIC-II)’ (Physionet repository). This database has simultaneous recordings of multi-parameters of Intensive care unit (ICU) patients which include physiological signals and parameters. For the purpose of this study, the simultaneous recordings of electro-cardiogram (ECG), photoplethysmograph (PPG) and arterial blood pressure (ABP) of 12000 subjects which are provided in the [UCI repository]('https://archive.ics.uci.edu/ml/datasets/Cuff-Less%2BBlood%2BPressure%2BEstimation') were used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cleaning the Data\n",
        "\n",
        "First, PPG and ECG data are pre-processed to eradicate the data of insufficient duration (less than 8 minutes recording), resulted in approximately 83% reduction in the dataset. Then, data segmentation is performed by taking 8 seconds window with 75% overlapping. Further, unreliable signals such as missing data (Nan), and very high/low BP and HR values (SBP ≥ 180, SBP ≤ 80, DBP ≥ 130, DBP ≤ 60, HR < 40,\n",
        "HR > 220 ) are excluded from the dataset, reduced the remaining data by approximately 20%. These cleaning steps result in reduction of total subjects from 12000 to 1557. This cleaned dataset of 1557 patients is used for the purpose of this study.\n",
        "\n",
        "This cleaning process was referred from the work done by Panwar et. al, \"PP-Net: A Deep Learning Framework for PPG-Based Blood Pressure and Heart Rate Estimation\".\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-24T10:15:04.264822Z",
          "start_time": "2021-11-24T10:13:23.660974Z"
        },
        "hidden": true,
        "id": "e_HbnSJWF8az"
      },
      "outputs": [],
      "source": [
        "li = []\n",
        "\n",
        "# The dataset has 12 parts\n",
        "\n",
        "for i in range (1,13):\n",
        "  path = 'your_path/dataset/PART_' + str(i)\n",
        "  number_of_patients = 0\n",
        "\n",
        "  filenames = glob.glob(path + \"/*.csv\")\n",
        "\n",
        "  for filename in filenames:\n",
        "      data = pd.read_csv(filename, names=['PPG', 'ABP', 'ECG'], index_col=None, header=0)\n",
        "      ## add the data cleaning part, and remove the outliers\n",
        "\n",
        "      abp_data = data[\"ABP\"]\n",
        "      number_of_patients += 1\n",
        "\n",
        "      if (data[\"ABP\"].size < 60000): ## Number of samples check, 60k = 8mins * 60s * 125Hz (atleast 8 mins footage)\n",
        "          continue\n",
        "  \n",
        "      peaks, _ = find_peaks(abp_data, distance = 40)\n",
        "      valleys, _ = find_peaks(-abp_data, distance = 40)\n",
        "      time_mins = (data[\"ABP\"].size/125)/60 #fs=125 Hz\n",
        "\n",
        "      if (np.mean(np.array(abp_data[peaks])) <= 180 and np.mean(np.array(abp_data[peaks])) >= 80 and np.mean(np.array(abp_data[valleys])) <=130 and np.mean(np.array(abp_data[valleys])) >= 60):\n",
        "          li.append(data)\n",
        "\n",
        "          address = str(i) + '/' + 'dataset_' + str(number_of_patients) + '.csv'\n",
        "          data.to_csv('your_path/clean_samples/PART_'+ address)\n",
        "\n",
        "df = pd.concat(li, axis=0, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-24T10:15:04.276392Z",
          "start_time": "2021-11-24T10:15:04.270308Z"
        },
        "hidden": true,
        "id": "fvZUltgWwJJj"
      },
      "outputs": [],
      "source": [
        "df.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yInpuCb5XVvr"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-24T10:16:24.516323Z",
          "start_time": "2021-11-24T10:15:04.282516Z"
        },
        "hidden": true,
        "id": "ojooX3k9zmJJ"
      },
      "outputs": [],
      "source": [
        "max_PPG = max(df['PPG'])\n",
        "max_ABP = max(df['ABP'])\n",
        "max_ECG = max(df['ECG'])\n",
        "\n",
        "df['PPG'] = df['PPG']/max_PPG\n",
        "df['ABP'] = df['ABP']/max_ABP\n",
        "df['ECG'] = df['ECG']/max_ECG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-24T10:16:24.521582Z",
          "start_time": "2021-11-24T10:16:24.518404Z"
        },
        "hidden": true,
        "id": "yys01tklKne3"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(df, columns = ['PPG','ABP','ECG'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "Rq__DCySplvF"
      },
      "source": [
        "### Obtaining Peaks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-24T10:17:04.475113Z",
          "start_time": "2021-11-24T10:16:24.523019Z"
        },
        "hidden": true,
        "id": "zxR4X30fC0nu"
      },
      "outputs": [],
      "source": [
        "lower = 0\n",
        "upper = df['ABP'].size\n",
        "abp = df['ABP'].to_numpy()\n",
        "abp = abp[lower:upper]\n",
        "\n",
        "peaks, _ = find_peaks(abp, distance=40)  # Systolic (110, 180)\n",
        "\n",
        "valleys, _ = find_peaks(-abp, distance=40) # Diastolic (40, 100)\n",
        "\n",
        "# plt.plot(abp)\n",
        "# plt.plot(peaks, abp[peaks], \"o\", color='r')\n",
        "# plt.plot(valleys, abp[valleys], \"o\", color='b')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpolating Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2021-11-24T10:13:20.257Z"
        },
        "hidden": true,
        "id": "YAIztsn5p3oN"
      },
      "outputs": [],
      "source": [
        "func = interpolate.interp1d(peaks, abp[peaks])\n",
        "\n",
        "x_vals_peaks = np.arange(peaks[0],peaks[len(peaks)-1])\n",
        "peaks_vals = func(x_vals_peaks)\n",
        "\n",
        "\n",
        "func = interpolate.interp1d(valleys, abp[valleys])\n",
        "\n",
        "x_vals_valleys = np.arange(valleys[0],valleys[len(valleys)-1])\n",
        "valleys_vals = func(x_vals_valleys)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "JVabI8D2p473"
      },
      "source": [
        "### Extending the Initial and Last Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2021-11-24T10:13:20.259Z"
        },
        "hidden": true,
        "id": "wpy7kcvZusrM"
      },
      "outputs": [],
      "source": [
        "x_vals = np.arange(0,upper)\n",
        "\n",
        "cont_peaks = np.zeros(upper)\n",
        "cont_peaks[0:peaks[0]] = peaks_vals[0]\n",
        "cont_peaks[peaks[0]:peaks[len(peaks)-1]] = peaks_vals[:]\n",
        "cont_peaks[peaks[len(peaks)-1]:] = peaks_vals[len(peaks_vals)-1]\n",
        "\n",
        "cont_valleys = np.zeros(upper)\n",
        "cont_valleys[0:valleys[0]] = valleys_vals[0]\n",
        "cont_valleys[valleys[0]:valleys[len(valleys)-1]] = valleys_vals[:]\n",
        "cont_valleys[valleys[len(valleys)-1]:] = valleys_vals[len(valleys_vals)-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2021-11-24T10:13:20.260Z"
        },
        "hidden": true,
        "id": "MrhXXIOJvh9A"
      },
      "outputs": [],
      "source": [
        "plt.plot(valleys, abp[valleys], 'o', x_vals, cont_valleys, color='r')\n",
        "plt.plot(peaks, abp[peaks], 'o', x_vals, cont_peaks, color='b')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2021-11-24T10:13:20.261Z"
        },
        "hidden": true,
        "id": "poUC4ctJ_SLr"
      },
      "outputs": [],
      "source": [
        "cont_peaks = cont_peaks.reshape(-1,1)\n",
        "cont_valleys = cont_valleys.reshape(-1,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2021-11-24T10:13:20.262Z"
        },
        "hidden": true,
        "id": "MgmTHeoez9Tx"
      },
      "outputs": [],
      "source": [
        "train_x = np.array(df.drop(columns=[\"ABP\"]))\n",
        "train_y = np.concatenate((cont_peaks, cont_valleys), axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrKxQDRO7IwN"
      },
      "source": [
        "### Preparing Data For Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6W3gO1qwXVvu"
      },
      "outputs": [],
      "source": [
        "time_window = 1000\n",
        "scaling_factor = 4\n",
        "input_size = int(time_window/scaling_factor)\n",
        "stride = int(input_size/4)\n",
        "\n",
        "from scipy.signal import butter\n",
        "from scipy.signal import sosfilt\n",
        "from scipy.signal import resample\n",
        "\n",
        "# denoising\n",
        "sos_ppg_up = butter(3, 20, 'low', fs=125, output='sos')\n",
        "sos_ppg_low = butter(3, 1.5, 'high', fs=125, output='sos')\n",
        "sos_ecg_up = butter(3, 40, 'low', fs=125, output='sos')\n",
        "sos_ecg_low = butter(3, 0.05, 'high', fs=125, output='sos')\n",
        "\n",
        "train_x[:,0] = sosfilt(sos_ppg_up, train_x[:,0])\n",
        "train_x[:,0] = sosfilt(sos_ppg_low, train_x[:,0])\n",
        "train_x[:,1] = sosfilt(sos_ecg_up, train_x[:,1])\n",
        "train_x[:,1] = sosfilt(sos_ecg_low, train_x[:,1])\n",
        "\n",
        "# downsampling\n",
        "train_x = resample(train_x, int(train_x.shape[0]/scaling_factor))\n",
        "train_y = resample(train_y, int(train_y.shape[0]/scaling_factor))\n",
        "\n",
        "print(train_x.shape)\n",
        "print(train_y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZYbE8F_XVvu"
      },
      "outputs": [],
      "source": [
        "dataset_x = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
        "    train_x, train_y, sequence_length=input_size, sequence_stride=stride, batch_size=1) ## /4 to get 25% different values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AprcccQXVvu"
      },
      "outputs": [],
      "source": [
        "dataset_y = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
        "    train_y, train_x, sequence_length=input_size, sequence_stride=stride, batch_size=1) ## /4 to get 25% different values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsijJAmSXVvu"
      },
      "outputs": [],
      "source": [
        "def dataset_to_numpy(ds):\n",
        "    \"\"\"\n",
        "    Convert tensorflow dataset to numpy arrays\n",
        "    \"\"\"\n",
        "    x = [train_x[:250]]\n",
        "    y = [train_y[:250]]\n",
        "   \n",
        "\n",
        "    # Iterate over a dataset\n",
        "    for i, (inp, out) in enumerate(tfds.as_numpy(ds)):\n",
        "        if i > 0:\n",
        "            inp = inp.reshape(input_size,2)\n",
        "            out = out.reshape(2)\n",
        "            x.append(inp)\n",
        "            y.append(out)\n",
        "        \n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHPktVo0XVvu"
      },
      "outputs": [],
      "source": [
        "x, _= dataset_to_numpy(dataset_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XL5W6fBjXVvv"
      },
      "outputs": [],
      "source": [
        "y, _= dataset_to_numpy(dataset_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7e8y80CAXVvv"
      },
      "outputs": [],
      "source": [
        "train_x = np.array(x)\n",
        "train_y = np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsJSnXzkXVvv"
      },
      "outputs": [],
      "source": [
        "filename = 'Data/75.pkl'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNznwY02XVvv"
      },
      "outputs": [],
      "source": [
        "with open(filename, 'wb') as f:\n",
        "    pickle.dump([train_x, train_y, max_ABP], f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1hLogJjXVvv"
      },
      "outputs": [],
      "source": [
        "train_x.shape\n",
        "# samples * 250 * 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFYH_pgPXVvv"
      },
      "outputs": [],
      "source": [
        "train_y.shape\n",
        "# samples * 2"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "mIY7ZiqMBIwK",
        "xpfQH0HpqGdM",
        "Rq__DCySplvF",
        "JVabI8D2p473",
        "H62fLeKbqVcs",
        "MrKxQDRO7IwN",
        "YdvXuuNOgZsP",
        "PU-7uPBXtcZ7",
        "URxODdivm2gZ",
        "b_bUZTBbrzAZ"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
